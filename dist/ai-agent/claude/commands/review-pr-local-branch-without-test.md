---
description: ãƒã‚§ãƒƒã‚¯ã‚¢ã‚¦ãƒˆã—ã¦ã„ã‚‹ãƒ­ãƒ¼ã‚«ãƒ«ãƒ–ãƒ©ãƒ³ãƒã«å¯¾ã™ã‚‹ãƒ—ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’å®Ÿæ–½ã™ã‚‹
---

# Review Pull Request on Local Branch

## Context

Review the pull request for the currently checked out local branch comprehensively.
Use the Task tool to conduct a multi-faceted review from multiple perspectives.

## Severity Classification

- **ğŸš¨ é‡å¤§(Critical)**: Could cause production failures, data loss, or security vulnerabilities. Blocks merge.
- **âš ï¸ æ”¹å–„ææ¡ˆ(Suggestion)**: Affects quality, maintainability, or performance but won't cause immediate failures. Further classified as:
  - **ä¿®æ­£å¿…é ˆ (Must Fix)**: Logic flaws, data integrity issues, or correctness problems that will cause bugs under realistic conditions.
  - **æ¬¡å›å¯¾å¿œå¯ (Can Address Later)**: Improves code quality but has no immediate functional impact.
- **ğŸ’¡ è»½å¾®(Nit)**: Code style, naming, comments. Does not block merge.

## Approval Decision Criteria

- **APPROVE**: Zero Critical findings AND zero "ä¿®æ­£å¿…é ˆ" Suggestions.
- **REQUEST_CHANGES**: One or more Critical findings, OR one or more "ä¿®æ­£å¿…é ˆ" Suggestions.
- **COMMENT**: No Critical or "ä¿®æ­£å¿…é ˆ" findings, but design decisions or architecture require discussion with the PR author.

## Review Principles

- **Report only high-confidence findings**: If unsure whether something is a problem, explicitly state the uncertainty and frame it as an observation. Prefer missing a real issue over reporting a false positive.
- **Respect intentional design decisions**: If a pattern is consistent across the codebase, treat it as intentional even if it looks unusual. Note as observation, not a problem.
- **Provide actionable feedback**: Every finding must include a concrete "how to fix" suggestion.
- **Avoid common false positive patterns**:
  - Framework-provided validation (e.g., Django/Rails model validators, Pydantic type coercion) flagged as "missing manual validation"
  - ORM query optimizations (e.g., Django `select_related`, SQLAlchemy lazy loading) flagged as "N+1 queries" when the ORM handles it
  - Hardcoded values in test code flagged as "security issues" or "magic numbers"
  - Intentionally broad exception handlers in top-level error boundaries flagged as "swallowing exceptions"
  - Optional parameters with `None` defaults flagged as "missing null checks" when the function explicitly handles `None`

## Procedure

### Step 1: PR Information Collection

1. Check current branch and retrieve PR metadata:
   ```bash
   git branch --show-current
   gh pr view --json number,title,body,baseRefName,url,labels,reviewRequests
   ```
2. Retrieve CI status and existing review comments:
   ```bash
   gh pr checks
   gh pr view --json reviews,comments
   ```
3. Get the diff, changed file list, and stats:
   ```bash
   BASE=$(gh pr view --json baseRefName -q .baseRefName)
   git diff "$BASE"...HEAD
   git diff --stat "$BASE"...HEAD
   git diff --name-only "$BASE"...HEAD
   ```
4. Read full content of changed files for surrounding context:
   ```bash
   for f in $(git diff --name-only "$BASE"...HEAD); do
     echo "=== $f ==="
     cat "$f" 2>/dev/null || echo "(file deleted)"
   done
   ```
   For large PRs (>30 files), limit full reads to files with substantial changes and read others on demand during subtasks.
5. Investigate callers and references of changed functions/classes using grep, ripgrep, or language-appropriate tools.
6. Find related test files for changed code.
7. Retrieve and collect information from URLs linked in the PR description and related issues/documents.

### Step 2: Understanding Pull Request Purpose

Based on collected information, produce the following summary for use by subtasks:

- **PR Purpose**: One-paragraph summary of intent and background
- **Changed Files**: Grouped by logical area
- **Full Diff**: Output from Step 1 (for large PRs, split by logical grouping)
- **Surrounding Context**: Key information from full file reads and caller analysis
- **Existing Test Coverage**: Summary of related test files found
- **CI Status**: Any failures or warnings
- **Existing Reviews**: Summary of prior review comments (subtasks must not duplicate these)
- **Related Context**: Key points from linked documents

### Step 3: Code Analysis and Review

Launch **6 concurrent subtasks** using the Task tool, one for each perspective below (A through F).

Each subtask receives the summary from Step 2, including the full diff (or relevant portions) and surrounding context.

**Common instructions for all subtasks**:
- Read the entire diff carefully and build a mental model of the change before analyzing.
- Consider not only what IS in the diff, but what SHOULD be there but is MISSING.
- For each finding, explain the **concrete failure scenario** (what input/condition triggers it, what happens).
- Classify each finding by severity using the Severity Classification defined above.
- Follow the Review Principles â€” especially avoiding false positive patterns listed above.
- Return a detailed report with file paths, line numbers, and specific issues or positive observations.

#### Subtask A â€” Logic Correctness & State Transitions
- Does the implementation meet the PR's purpose and requirements?
- Are algorithms and data flows correct?
- Is error handling sufficient across all error paths?
- Are there off-by-one errors, race conditions, or infinite loops?
- **State transitions**: If the code involves entities with state (e.g., status fields, workflow stages), are all valid transitions enforced? Are invalid transitions prevented?
- **What is missing?** Are there requirements from the PR description that are not implemented?

#### Subtask B â€” Input Validation & Data Integrity
- Data type/format validation (dates, string lengths, numeric ranges, enums)
- Domain-specific business rule validation and cross-field consistency
- Data integrity (duplicate prevention, foreign key validity, permission checks)
- Edge cases: NULL/empty string/zero handling, boundary values, error messages

#### Subtask C â€” Maintainability and Readability
- Code structure clarity and naming consistency
- Function/class responsibility (Single Responsibility)
- Comment appropriateness
- Code duplication and abstraction level

#### Subtask D â€” Security
- Authentication/authorization correctness
- Injection vulnerabilities (SQL, command, template, etc.)
- Sensitive information leakage (logs, error messages, responses)
- Secrets/credentials handling
- CSRF/XSS prevention (if applicable)
- Note: Business-logic input validation is covered by Subtask B. Focus here on security-specific attack vectors.

#### Subtask E â€” Performance and Scalability
- N+1 query problems or unnecessary database calls (verify the ORM does not already optimize before flagging)
- Inefficient loops or redundant computations
- Memory usage (large data loading, unbounded collections)
- Missing indexes or inefficient queries
- Scalability with expected data growth
- Concurrency issues (race conditions, deadlocks)

#### Subtask F â€” Test Quality
- Are normal case tests sufficient for the changed code?
- **Cross-check with changed code**: For each validation rule or business logic branch in the changed code, is there a corresponding test for invalid input, boundary values, and edge cases?
- Are error paths and failure modes tested?
- Is test data appropriate and realistic?
- Are there missing test cases? If no tests were added, do existing tests adequately cover the changes?

**Wait for all 6 subtasks to complete before proceeding.**

### Step 4: Aggregate and Determine Approval

1. **Merge findings** from all subtasks, deduplicating overlapping issues.
2. **Cross-check subtask findings**:
   - For each validation issue from Subtask B, verify whether Subtask F found corresponding test coverage.
   - If subtasks disagree on a finding, investigate the actual code context and make a final determination.
3. **Eliminate false positives**: Re-examine each Critical and "ä¿®æ­£å¿…é ˆ" Suggestion against actual code and surrounding context. Downgrade or remove findings that:
   - Are based on misunderstanding of the code's intent
   - Are inconsistent with established codebase patterns
   - Cannot be triggered under realistic conditions
   - Match the common false positive patterns listed in Review Principles
4. **Check coherence**: Ensure findings do not contradict each other.
5. **Filter existing feedback**: Remove findings that duplicate prior review comments.
6. Sort findings by severity (Critical â†’ Suggestion â†’ Nit).
7. Determine approval status based on the Approval Decision Criteria.

### Step 5: Output Review Results

Generate a Markdown file in the following format **in Japanese**:

```markdown
# ãƒ—ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ¬ãƒ“ãƒ¥ãƒ¼çµæœ

## åŸºæœ¬æƒ…å ±
- **ãƒ–ãƒ©ãƒ³ãƒ**: [branch name]
- **ãƒ—ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆ**: [PR number and title]
- **ç›®çš„**: [summary of PR purpose]

## å¤‰æ›´æ¦‚è¦
[å¤‰æ›´ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã€è¡Œæ•°ãªã©ã®çµ±è¨ˆã‚’å«ã‚€å¤‰æ›´ã®æ¦‚è¦]

## ãƒ¬ãƒ“ãƒ¥ãƒ¼çµæœ

### âœ… è‰¯ã„ç‚¹
[ç‰¹ç­†ã™ã¹ãè‰¯ã„å®Ÿè£…ã‚’2ã€œ3ç‚¹ã«çµã£ã¦è¨˜è¼‰ã€‚ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’å«ã‚ã‚‹ã€‚]

### ğŸš¨ é‡å¤§ãªå•é¡Œ
[ç•ªå·ä»˜ãã§è¨˜è¼‰ã€‚è©²å½“ãªã—ã®å ´åˆã¯ã€Œãªã—ã€ã¨è¨˜è¼‰ã€‚]

1. **ãƒ•ã‚¡ã‚¤ãƒ«**: `path/to/file.py:è¡Œç•ªå·`
   - **å•é¡Œ**: [å•é¡Œã®èª¬æ˜]
   - **å½±éŸ¿**: [å…·ä½“çš„ãªéšœå®³ã‚·ãƒŠãƒªã‚ª: ã©ã®ã‚ˆã†ãªå…¥åŠ›ãƒ»æ¡ä»¶ã§ç™ºç”Ÿã—ã€ä½•ãŒèµ·ã“ã‚‹ã‹]
   - **ææ¡ˆ**: [å…·ä½“çš„ãªä¿®æ­£æ–¹æ³•]
   - **diffä¾‹**:
     ```diff
     - [ä¿®æ­£å‰ã‚³ãƒ¼ãƒ‰]
     + [ä¿®æ­£å¾Œã‚³ãƒ¼ãƒ‰]
     ```

### âš ï¸ æ”¹å–„ææ¡ˆ
[ç•ªå·ä»˜ãã§è¨˜è¼‰ã€‚å„é …ç›®ã«ã€Œä¿®æ­£å¿…é ˆã€ã¾ãŸã¯ã€Œæ¬¡å›å¯¾å¿œå¯ã€ã‚’æ˜ç¤ºã€‚è©²å½“ãªã—ã®å ´åˆã¯ã€Œãªã—ã€ã¨è¨˜è¼‰ã€‚]

1. **ãƒ•ã‚¡ã‚¤ãƒ«**: `path/to/file.py:è¡Œç•ªå·` â€” ä¿®æ­£å¿…é ˆ / æ¬¡å›å¯¾å¿œå¯
   - **å•é¡Œ**: [å•é¡Œã®èª¬æ˜]
   - **å½±éŸ¿**: [å½±éŸ¿ã®èª¬æ˜]
   - **ææ¡ˆ**: [å…·ä½“çš„ãªä¿®æ­£æ–¹æ³•]
   - **diffä¾‹**:
     ```diff
     - [ä¿®æ­£å‰ã‚³ãƒ¼ãƒ‰]
     + [ä¿®æ­£å¾Œã‚³ãƒ¼ãƒ‰]
     ```

### ğŸ’¡ è»½å¾®ãªæŒ‡æ‘˜
[è©²å½“ãªã—ã®å ´åˆã¯ã€Œãªã—ã€ã¨è¨˜è¼‰ã€‚]

- **ãƒ•ã‚¡ã‚¤ãƒ«**: `path/to/file.py:è¡Œç•ªå·`
  - **å†…å®¹**: [æŒ‡æ‘˜å†…å®¹]

## ç·åˆè©•ä¾¡
- **æ‰¿èªå¯å¦**: [APPROVE / REQUEST_CHANGES / COMMENT]
- **ç†ç”±**: [æ‰¿èªåˆ¤å®šåŸºæº–ã«åŸºã¥ãåˆ¤å®šç†ç”±]
- **é‡å¤§ãªå•é¡Œ**: [N]ä»¶ / **æ”¹å–„ææ¡ˆ**: [N]ä»¶ï¼ˆä¿®æ­£å¿…é ˆ: [N]ä»¶, æ¬¡å›å¯¾å¿œå¯: [N]ä»¶ï¼‰ / **è»½å¾®ãªæŒ‡æ‘˜**: [N]ä»¶
```

## Output Requirements
- All findings must include file paths and line numbers.
- All Critical and Suggestion findings must include concrete code examples in diff format.
- All Critical and Suggestion findings must describe a concrete failure scenario.
- Every Suggestion must be classified as "ä¿®æ­£å¿…é ˆ" or "æ¬¡å›å¯¾å¿œå¯".
- Limit "è‰¯ã„ç‚¹" to 2-3 noteworthy items.
- Approval status must be justified with explicit reference to the Approval Decision Criteria.
- Do not duplicate feedback already provided by other reviewers.